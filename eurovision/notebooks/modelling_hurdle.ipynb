{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('../../data/df_main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['year'] == 2004) & (df['from_code2'] == 'GB')].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost to predict if score given\n",
    "\n",
    "First step in the hurdle model.\n",
    "\n",
    "Example: https://www.kaggle.com/code/sriharinitumu/light-gbm-with-hurdle-modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['received_vote'] = df['points'].apply(lambda x: 1 if x > 0 else 0)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "\n",
    "# Bar chart for vote occurrences\n",
    "sns.countplot(x=\"points\", data=df, ax=ax1)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_xlabel('Score', fontsize=12)\n",
    "ax1.set_title('Number of votes', fontsize=12)\n",
    "\n",
    "sns.countplot(x=\"received_vote\", data=df, ax=ax2)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_xlabel('Received vote', fontsize=12)\n",
    "ax2.set_title('Number of votes', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have a lot of zeroes....this probably makes this problem appropriate for a hurdle model. Where we first decide if a score is given, and then if it is, we predict the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale some of the features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df_xgboost = df[['year', 'from_code2', 'points', 'to_code2',\n",
    "        'Contains_English',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'Contains_Voting_Language', 'gender',\n",
    "       'prop_emigrants_v2p',  'prop_emigrants_p2v', 'has_border',\n",
    "       'comps_without_win', 'received_vote']]\n",
    "\n",
    "df_xgboost['has_border'] = df_xgboost['has_border'].fillna(0)\n",
    "\n",
    "# log10 scale the prop_emigrants column\n",
    "df_xgboost['prop_emigrants_v2p'] = df_xgboost['prop_emigrants_v2p'].apply(lambda x: np.log10(x+4e-8))\n",
    "df_xgboost['prop_emigrants_p2v'] = df_xgboost['prop_emigrants_p2v'].apply(lambda x: np.log10(x+4e-8))\n",
    "\n",
    "\n",
    "# apply the standard scaler to prop_emigrants and comps_without_win\n",
    "scaler = StandardScaler()\n",
    "df_xgboost[['prop_emigrants_v2p']] = scaler.fit_transform(df_xgboost[['prop_emigrants_v2p']])\n",
    "df_xgboost[['prop_emigrants_p2v']] = scaler.fit_transform(df_xgboost[['prop_emigrants_p2v']])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_xgboost[['comps_without_win']] = scaler.fit_transform(df_xgboost[['comps_without_win']])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# histogram of the log scaled prop_emigrants column\n",
    "df_xgboost['comps_without_win'].hist(bins=20, ax=ax1)\n",
    "df_xgboost['prop_emigrants_v2p'].hist(bins=200, ax=ax2)\n",
    "df_xgboost['prop_emigrants_p2v'].hist(bins=200, ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import loadtxt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def basic_xgboost(df_input, seed, test_size, predictor_column='received_vote'):\n",
    "    X = df_input[['from_code2', 'to_code2', 'points',\n",
    "        'Contains_English',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'Contains_Voting_Language', 'gender',\n",
    "       'prop_emigrants_v2p',  'prop_emigrants_p2v','has_border',\n",
    "       'comps_without_win', \"received_vote\"]]\n",
    "\n",
    "\n",
    "\n",
    "    Y = df_input[predictor_column]\n",
    "\n",
    "\n",
    "    # one hot encode the gender, from country and code\n",
    "    for j in ['gender', 'from_code2', 'to_code2']:\n",
    "        j_text = '_voting' if j == 'from_code2' else ''\n",
    "        for i in X[j].unique():\n",
    "            output_binary = X[j].apply(lambda x: 1 if x == i else 0)\n",
    "            X[i+j_text] = output_binary\n",
    "\n",
    "        X = X[X.columns.drop(j)]\n",
    "\n",
    "    # convert Y to int\n",
    "    Y = Y.astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed, stratify=X['points'])\n",
    "\n",
    "    test_all = X_test\n",
    "\n",
    "    # remove 'Votes' and 'received_vote' columns from X_train\n",
    "    X_train = X_train[X_train.columns.drop('points')]\n",
    "    X_train = X_train[X_train.columns.drop('received_vote')]\n",
    "    X_test = X_test[X_test.columns.drop('points')]\n",
    "    X_test = X_test[X_test.columns.drop('received_vote')]\n",
    "\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_test, y_test, test_all\n",
    "\n",
    "def model_predictions(model, X_test, y_test):\n",
    "    Xt = X_test.copy()\n",
    "    y_pred = model.predict(Xt)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    Xt['prob'] = model.predict_proba(Xt)[:,1]\n",
    "    Xt['predictions'] = predictions\n",
    "    Xt['actual'] = y_test\n",
    "    return Xt\n",
    "\n",
    "def model_evalutation(df_pred):\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(df_pred['actual'], df_pred['predictions'])\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "    # print a confusion matrix heatmap\n",
    "\n",
    "    cm = confusion_matrix(df_pred['actual'], df_pred['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "    # add title and axis labels\n",
    "    plt.title('Confusion matrix' + \"(Accuracy={:.3f})\".format(accuracy))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basic, X_test, y_test, test_all = basic_xgboost(df_xgboost, seed=7, test_size=0.33)\n",
    "predictions = model_predictions(model_basic, X_test, y_test)\n",
    "model_evalutation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  test_all.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "\n",
    "graph = xgb.to_graphviz(model_basic, num_trees=1, rankdir='LR')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 50))\n",
    "xgb.plot_tree(model_basic, num_trees=1, ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Scores\n",
    "\n",
    "So of those, lets subset to only those with a score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def xgboost_rank_model(df_input, test_size=0.33, seed=7):\n",
    "\n",
    "    df2 = df_input[['year', 'from_code2', 'to_code2', 'points',\n",
    "        'Contains_English',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'Contains_Voting_Language', 'gender',\n",
    "       'prop_emigrants_v2p',  'prop_emigrants_p2v','has_border',\n",
    "       'comps_without_win']]\n",
    "\n",
    "    #df2 = df2.loc[df2['Votes'] > 0]\n",
    "\n",
    "    #df2['has_border'] = df2['has_border'].fillna(0)\n",
    "    df2 = df2.rename(columns={\"points\": 'rank'})\n",
    "    #df2 = df2.rename(columns={\"From country\": 'id'})\n",
    "\n",
    "    df2['rank'] = df2['rank'].astype(int) - 1\n",
    "\n",
    "    df2['id'] = df2['from_code2'].astype(str) + df2['year'].astype(str)\n",
    "\n",
    "    # one hot encode the gender, from country and code\n",
    "    for j in ['gender', 'to_code2', 'from_code2']:\n",
    "        j_text = \"_voting\" if j == 'from_code2' else ''\n",
    "        for i in df2[j].unique():\n",
    "            df2[i+j_text] = df2[j].apply(lambda x: 1 if x == i else 0)\n",
    "\n",
    "        df2 = df2[df2.columns.drop(j)]\n",
    "\n",
    "    #df2= df2[df2.columns.drop('From country')]\n",
    "    df2= df2[df2.columns.drop('year')]\n",
    "\n",
    "    gss = GroupShuffleSplit(test_size=test_size, n_splits=1, random_state = seed).split(df2, groups=df2['id'])\n",
    "\n",
    "    X_train_inds, X_test_inds = next(gss)\n",
    "\n",
    "    train_data= df2.iloc[X_train_inds]\n",
    "    X_train = train_data.loc[:, ~train_data.columns.isin(['id','rank'])]\n",
    "    y_train = train_data.loc[:, train_data.columns.isin(['rank'])]\n",
    "\n",
    "    groups = train_data.groupby('id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "    test_data= df2.iloc[X_test_inds]\n",
    "    test_data = test_data[X_train.columns.tolist() + ['rank', 'id']]\n",
    "\n",
    "    #We need to keep the id for later predictions\n",
    "    # X_test = test_data.loc[:, ~test_data.columns.isin(['rank'])]\n",
    "    # y_test = test_data.loc[:, test_data.columns.isin(['rank'])]\n",
    "\n",
    "    model = xgb.XGBRanker(  \n",
    "        tree_method='hist',\n",
    "        booster='gbtree',\n",
    "        objective='rank:pairwise',\n",
    "        random_state=7, \n",
    "        learning_rate=0.1,\n",
    "        colsample_bytree=0.9, \n",
    "        eta=0.05, \n",
    "        max_depth=6, \n",
    "        n_estimators=110, \n",
    "        subsample=0.75 \n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, group=groups, verbose=True)\n",
    "\n",
    "    return model, test_data, train_data\n",
    "\n",
    " \n",
    "\n",
    "def ranked_model_predictions(model, test_set):\n",
    "    test = test_set.copy()\n",
    "    \n",
    "    test['prediction_rel'] = np.nan\n",
    "    test['predictions'] = np.nan\n",
    "\n",
    "    for i in test['id'].unique():\n",
    "        sub_test = test.loc[test['id'] == i, ~test.columns.isin(['id', 'rank', 'prediction_rel', 'predictions'])]\n",
    "\n",
    "        preds = model.predict(sub_test)\n",
    "\n",
    "        # get the order of the predictions \n",
    "        res = rankdata(preds, method='ordinal')\n",
    "        test.loc[test['id'] == i, 'prediction_rel'] = res\n",
    "\n",
    "        # create a score for top 10 predictions getting 1:10 and others 0\n",
    "        top10 = test.loc[test['id'] == i, 'prediction_rel'].nlargest(10).values\n",
    "\n",
    "\n",
    "        test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'prediction_rel'].apply(lambda x: x if x in top10 else 0)\n",
    "        \n",
    "        # if not zero subtract min(top10) \n",
    "        test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'predictions'].apply(lambda x: x - max(top10) + 10 if x != 0 else 0)\n",
    "\n",
    "        # if 10 set to 12\n",
    "        test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'predictions'].apply(lambda x: 12 if x == 10 else x)\n",
    "        test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'predictions'].apply(lambda x: 10 if x == 9 else x)\n",
    "\n",
    "    test['actual'] = test['rank'] + 1\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(test['actual'], test['predictions'])\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ranked, test_data, train_data = xgboost_rank_model(df_xgboost.loc[df_xgboost['points'] > 0], seed=7, test_size=0.33)\n",
    "out = ranked_model_predictions(model_ranked, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ranked_all, test_data_all, train_data_all = xgboost_rank_model(df_xgboost, seed=7, test_size=0.33)\n",
    "out_all = ranked_model_predictions(model_ranked_all, test_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_all['predictions'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violins(pred):\n",
    "    prediction_df = pred.copy()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7)) \n",
    "\n",
    "    if(max(prediction_df['actual']) < 12):    \n",
    "        if(max(prediction_df['actual']) == 11):\n",
    "            prediction_df['predictions'] = prediction_df['predictions'] + 1\n",
    "            prediction_df['actual'] = prediction_df['actual'] + 1\n",
    "        prediction_df['predictions'] = prediction_df['predictions'].apply(lambda x: 12 if x == 10 else x)\n",
    "        prediction_df['predictions'] = prediction_df['predictions'].apply(lambda x: 10 if x == 9 else x)\n",
    "        prediction_df['actual'] = prediction_df['actual'].apply(lambda x: 12 if x == 10 else x)\n",
    "        prediction_df['actual'] = prediction_df['actual'].apply(lambda x: 10 if x == 9 else x)\n",
    "\n",
    "    # violin plot of predictions for each rank\n",
    "    sns.violinplot(x=\"actual\", y=\"predictions\", data=prediction_df, order=range(0, 13), ax=ax1)\n",
    "    sns.violinplot(y=\"actual\", x=\"predictions\", data=prediction_df, order=range(0, 13), ax=ax2)\n",
    "\n",
    "    # add a best fit line - scale is wrong because numeric on top of categorical\n",
    "    sns.regplot(x=\"actual\", y=\"predictions\", data=prediction_df, scatter=False, color='black', ax=ax1)\n",
    "    sns.regplot(y=\"actual\", x=\"predictions\", data=prediction_df, scatter=False, color='black', ax=ax2)\n",
    "\n",
    "\n",
    "    # spearman correlation\n",
    "    corr, _ = spearmanr(prediction_df['actual'], prediction_df['predictions'])\n",
    "\n",
    "    fig.suptitle('Predicted Score vs Actual Score (r = ' + str(round(corr, 4)) + ')', fontsize=15)\n",
    "\n",
    "    # set x label\n",
    "    ax1.set_xlabel('Actual Score')\n",
    "    ax1.set_ylabel('Predicted Score')\n",
    "    ax2.set_ylabel('Actual Score')\n",
    "    ax2.set_xlabel('Predicted Score')\n",
    "\n",
    "def cm_heatmap (prediction_df, title='Confusion matrix', ax=None):\n",
    "    cm = confusion_matrix(prediction_df['actual'], prediction_df['predictions'])\n",
    "\n",
    "    plot = sns.heatmap(cm, annot=True, fmt='g', ax=ax, vmin=0, vmax=100)\n",
    "\n",
    "    if(len(cm[[0]][0]) == 11) : \n",
    "    # change the tick labels\n",
    "        plot.set_xticks(np.arange(0, 11, 1)+0.5, np.arange(0, 9, 1).tolist() + [10, 12])\n",
    "        plot.set_yticks(np.arange(0, 11, 1)+0.5, np.arange(0, 9, 1).tolist() + [10, 12])\n",
    "    else: \n",
    "        plot.set_xticks(np.arange(0, 10, 1)+0.5, np.arange(1, 9, 1).tolist() + [10, 12])\n",
    "        plot.set_yticks(np.arange(0, 10, 1)+0.5, np.arange(1, 9, 1).tolist() + [10, 12])\n",
    "\n",
    "    plot.set_title(title)\n",
    "    plot.set_ylabel('Actual Score')\n",
    "    plot.set_xlabel('Predicted Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violins(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violins(out_all.loc[out_all['actual'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_heatmap(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_heatmap(out_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://discuss.xgboost.ai/t/evaluating-xgboost-ranking/959/2) for description of prediction scores\n",
    "\n",
    "The 0 predictions are coming up when we have more that 10 non-zeroes (due to draws). So this actually works quite well I think.\n",
    "\n",
    "### Quickly see how this compares to a non pairwise rank model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes = df_xgboost.loc[df_xgboost['points'] > 0]\n",
    "\n",
    "df_votes['Votes2'] = df_votes[\"points\"] - 1\n",
    "df_votes['Votes2'] = df_votes['Votes2'].apply(lambda x: 8 if x == 9 else x)\n",
    "df_votes['Votes2'] = df_votes['Votes2'].apply(lambda x: 9 if x == 11 else x)\n",
    "\n",
    "model, X_test, y_test, t2 = basic_xgboost(df_votes, seed=7, test_size=0.33, predictor_column='Votes2')\n",
    "predictions = model_predictions(model, X_test, y_test)\n",
    "model_evalutation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violins(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked and unranked plot side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n",
    "fig.suptitle('Confusion Matrix for Ranked and Unranked Predictions')\n",
    "\n",
    "# ranked plot\n",
    "cm_heatmap(out, title='Ranked Predictions', ax=ax1)\n",
    "cm_heatmap(predictions, title='Unranked Predictions', ax=ax2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the hurdles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost['id'] = df_xgboost['from_code2'] + df_xgboost['year'].astype(str)\n",
    "\n",
    "df_xgboost['rank'] = df_xgboost['points']\n",
    "\n",
    "df_xgboost['rank'] = df_xgboost['rank'].apply(lambda x: 9 if x == 10 else x)\n",
    "df_xgboost['rank'] = df_xgboost['rank'].apply(lambda x: 10 if x == 12 else x)\n",
    "\n",
    "#df_xgboost['rank'].value_counts()\n",
    "df_xgboost['rank'] = df_xgboost['rank'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hurdle = df_xgboost.copy()\n",
    "\n",
    "#df_hurdle['has_border'] = df_hurdle['has_border'].fillna(0)\n",
    "\n",
    "df_hurdle.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the gender, from country and code\n",
    "for j in ['gender', 'to_code2', 'from_code2']:\n",
    "    j_text = \"_voting\" if j == 'from_code2' else ''\n",
    "    for i in df_hurdle[j].unique():\n",
    "        df_hurdle[i+j_text] = df_hurdle[j].apply(lambda x: 1 if x == i else 0)\n",
    "\n",
    "    df_hurdle = df_hurdle[df_hurdle.columns.drop(j)]\n",
    "\n",
    "#df2= df2[df2.columns.drop('From country')]\n",
    "df_hurdle= df_hurdle[df_hurdle.columns.drop('year')]\n",
    "\n",
    "df_hurdle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# select a random sample of 33% of the unique values of df_hurdle['id']\n",
    "test_groups = np.random.choice(df_hurdle['id'].unique(), size=int(len(df_hurdle['id'].unique())*0.33), replace=False)\n",
    "train_groups = np.setdiff1d(df_hurdle['id'].unique(), test_groups)\n",
    "\n",
    "train_data = df_hurdle.loc[df_hurdle['id'].isin(train_groups)]\n",
    "test_data = df_hurdle.loc[df_hurdle['id'].isin(test_groups)]\n",
    "\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['id','rank', 'points', 'received_vote'])]\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['received_vote'])]\n",
    "\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['id','rank', 'points', 'received_vote'])]\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['received_vote'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "hurdle1_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurdle1_df = model_predictions(hurdle1_model, X_test,  y_test)\n",
    "\n",
    "# merge test_data and hurdle1_df\n",
    "cols = [i for i in test_data.columns if i not in hurdle1_df.columns]\n",
    "hurdle1_df = pd.merge(hurdle1_df, test_data[cols], left_index=True, right_index=True)\n",
    "\n",
    "hurdle1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evalutation(hurdle1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = hurdle1_df.loc[hurdle1_df['predictions'] > 0]\n",
    "\n",
    "# remove predictions, actual and received_vote columns\n",
    "df2 = df2[df2.columns.drop(['predictions', 'actual', \"points\", 'received_vote'])]\n",
    "\n",
    "print(df2['rank'].value_counts())\n",
    "\n",
    "#df2 = df2.rename(columns={\"Votes\": 'rank'})\n",
    "#df2 = df2.rename(columns={\"From country\": 'id'})\n",
    "\n",
    "#df2['rank'] = df2['rank'].astype(int) - 1\n",
    "\n",
    "\n",
    "# select a random sample of 33% of the unique values of df_hurdle['id']\n",
    "test_groups = np.random.choice(df2['id'].unique(), size=int(len(df2['id'].unique())*0.33), replace=False)\n",
    "train_groups = np.setdiff1d(df2['id'].unique(), test_groups)\n",
    "\n",
    "train_data = df2.loc[df2['id'].isin(train_groups)]\n",
    "test_data = df2.loc[df2['id'].isin(test_groups)]\n",
    "\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['id','rank', 'points', 'received_vote'])]\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['rank'])]\n",
    "\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['id','rank', 'points', 'received_vote'])]\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['rank'])]\n",
    "\n",
    "print(test_data['id'].value_counts())\n",
    "print(train_data['id'].value_counts())\n",
    "\n",
    "groups = train_data.groupby('id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "model_ranked = xgb.XGBRanker(  \n",
    "    tree_method='hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:pairwise',\n",
    "    random_state=7, \n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.9, \n",
    "    eta=0.05, \n",
    "    max_depth=6, \n",
    "    n_estimators=110, \n",
    "    subsample=0.75 \n",
    ")\n",
    "\n",
    "model_ranked.fit(X_train, y_train, group=groups, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test = test_data.copy()\n",
    "\n",
    "test['prediction_rel'] = np.nan\n",
    "test['predictions'] = np.nan\n",
    "\n",
    "for i in test['id'].unique():\n",
    "    sub_test = test.loc[test['id'] == i, ~test.columns.isin(['id', 'rank', 'prediction_rel', 'predictions'])]\n",
    "\n",
    "    preds = model.predict(sub_test)\n",
    "\n",
    "    # get the order of the predictions \n",
    "    res = rankdata(preds, method='ordinal')\n",
    "    test.loc[test['id'] == i, 'prediction_rel'] = res\n",
    "\n",
    "    # create a score for top 10 predictions getting 1:10 and others 0\n",
    "    top10 = test.loc[test['id'] == i, 'prediction_rel'].nlargest(10).values\n",
    "\n",
    "\n",
    "    test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'prediction_rel'].apply(lambda x: x if x in top10 else 0)\n",
    "    \n",
    "    # if not zero subtract min(top10) \n",
    "    test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'predictions'].apply(lambda x: x - max(top10) + 10 if x != 0 else 0)\n",
    "\n",
    "    # if 10 set to 12\n",
    "    #test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'predictions'].apply(lambda x: 12 if x == 10 else x)\n",
    "    #test.loc[test['id'] == i, 'predictions'] = test.loc[test['id'] == i, 'predictions'].apply(lambda x: 10 if x == 9 else x)\n",
    "\n",
    "test['actual'] = test['rank'] \n",
    "\n",
    "\n",
    "accuracy = accuracy_score(test['actual'], test['predictions'])\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "predictions2 = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2['actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hurdle1_df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = hurdle1_df.copy()\n",
    "\n",
    "\n",
    "cols = [col for col in p1.columns if '_voting' in col]\n",
    "cols2 = [col for col in p1.columns if '_voting' not in col]\n",
    "\n",
    "\n",
    "# wide format _voting columns to long format\n",
    "hurdle_input2 = pd.melt(p1, id_vars=cols2, \n",
    "                       value_vars= cols, var_name='from_code2')\n",
    "\n",
    "hurdle_input2 = hurdle_input2.loc[hurdle_input2['value'] ==1]\n",
    "\n",
    "\n",
    "\n",
    "p1['from_code2'] = hurdle_input2['from_code2'].str.replace('_voting', '').tolist()\n",
    "print(p1['from_code2'])\n",
    "p1 = p1.drop(cols, axis=1)\n",
    "\n",
    "\n",
    "cols2 = [#'year', \n",
    "       'from_code2','received_vote', #'to_code2', \n",
    "       'points', 'rank',\n",
    "       'Contains_English', 'id',\n",
    "       'has_border',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'Contains_Voting_Language', \n",
    "       'prop_emigrants_v2p',  'prop_emigrants_p2v', \n",
    "       'comps_without_win', 'group', 'female', 'male', \n",
    "       'predictions', 'actual']\n",
    "\n",
    "cols = [col for col in p1.columns if col not in cols2]\n",
    "\n",
    "# print p1.columns not in cols2\n",
    "print([col for col in p1.columns if col not in cols2])\n",
    "\n",
    "hurdle_input4 = pd.melt(p1, id_vars=cols2, \n",
    "                       value_vars= cols, var_name='to_code2')\n",
    "\n",
    "hurdle_input4 = hurdle_input4.loc[hurdle_input4['value'] == 1]\n",
    "\n",
    "# add hurdle_input2['From country'] to hurdle_input\n",
    "\n",
    "p1['to_code2'] = hurdle_input4['to_code2'].tolist()\n",
    "\n",
    "predictions1 = p1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = predictions1.loc[predictions1['predictions'] == 0]\n",
    "\n",
    "#print(pred1['actual'].value_counts())\n",
    "\n",
    "pred1 = pred1[['predictions', 'rank', 'id']] # , 'code', 'From country'\n",
    "\n",
    "# rename Votes as actual\n",
    "pred1 = pred1.rename(columns={\"rank\": 'actual'})\n",
    "\n",
    "pred2 = predictions2[['predictions', 'actual', 'id']]\n",
    "\n",
    "# order by id\n",
    "pred2 = pred2.sort_values(by=['id'])\n",
    "\n",
    "#pred2 = pred2.rename(columns={\"rank\": 'actual'})\n",
    "\n",
    "# pred2['code'] = pred2['id'].str[:2]\n",
    "# pred2['Year'] = pred2['id'].str[-4:]\n",
    "# pred2 = pred2.drop(columns=['id'])\n",
    "\n",
    "pred2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes\n",
    "predictions_final = pd.concat([pred1, pred2])\n",
    "\n",
    "predictions_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violins(predictions_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# confusion matrix for hurdle model\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(predictions_final['predictions'], predictions_final['actual'])\n",
    "\n",
    "cm = confusion_matrix(y_true=predictions_final['actual'], y_pred=predictions_final['predictions'])\n",
    "plot = sns.heatmap(cm, annot=True, fmt='g', vmin=0, vmax=75)\n",
    "\n",
    "print(len(cm[[0]][0]))\n",
    "\n",
    "if(len(cm[[0]][0]) == 11) : \n",
    "# change the tick labels\n",
    "    plot.set_xticks(np.arange(0, 11, 1)+0.5, np.arange(0, 9, 1).tolist() + [10, 12])\n",
    "    plot.set_yticks(np.arange(0, 11, 1)+0.5, np.arange(0, 9, 1).tolist() + [10, 12])\n",
    "else: \n",
    "    plot.set_xticks(np.arange(0, 10, 1)+0.5, np.arange(1, 9, 1).tolist() + [10, 12])\n",
    "    plot.set_yticks(np.arange(0, 10, 1)+0.5, np.arange(1, 9, 1).tolist() + [10, 12])\n",
    "\n",
    "# spearman correlation\n",
    "corr, p = spearmanr(predictions_final['actual'], predictions_final['predictions'])\n",
    "\n",
    "plot.text(x=6, y=-1.5, s= \"XGBoost hurdle model\", fontsize=12, ha=\"center\")\n",
    "plot.text(x=6, y=-0.5, s= \"Accuracy=%.2f%%, r=%.2f\"% (accuracy * 100.0, corr), fontsize=12, ha=\"center\")\n",
    "plot.set_ylabel('Actual Score')\n",
    "plot.set_xlabel('Predicted Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the 2023 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv\n",
    "df_2023 = pd.read_csv('../../data/df_2023.csv')\n",
    "\n",
    "\n",
    "df_2023['id'] = df_2023['from_code2'] + df_2023['year'].astype(str)\n",
    "df_2023['received_vote'] = 0\n",
    "\n",
    "df_2023 = df_2023[df_xgboost.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023['from_code2'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove russia from the voting\n",
    "df_2023 = df_2023.loc[df_2023['from_code2'] != 'RU', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023['from_code2']\n",
    "\n",
    "df_2023['id'] = df_2023['from_code2'] + df_2023['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hurdle = df_2023.copy()\n",
    "\n",
    "# one hot encode the gender, from country and code\n",
    "for j in ['gender', 'to_code2', 'from_code2']:\n",
    "    j_text = \"_voting\" if j == 'from_code2' else ''\n",
    "    for i in df_hurdle[j].unique():\n",
    "        df_hurdle[i+j_text] = df_hurdle[j].apply(lambda x: 1 if x == i else 0)\n",
    "\n",
    "    df_hurdle = df_hurdle[df_hurdle.columns.drop(j)]\n",
    "\n",
    "#df2= df2[df2.columns.drop('From country')]\n",
    "df_hurdle= df_hurdle[df_hurdle.columns.drop('year')]\n",
    "\n",
    "df_hurdle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the test_data_all columns which are not in df_hurdle\n",
    "print([col for col in df_hurdle.columns if col not in test_data_all.columns])\n",
    "print([col for col in test_data_all.columns if col not in df_hurdle.columns])\n",
    "\n",
    "# add the missing columns\n",
    "for col in test_data_all.columns:\n",
    "    if col not in df_hurdle.columns:\n",
    "        df_hurdle[col] = 0\n",
    "\n",
    "df_hurdle = df_hurdle[test_data_all.columns]\n",
    "\n",
    "df_hurdle['rank'] = 1\n",
    "\n",
    "df_hurdle.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ranked_all, test_data_all, train_data_all = xgboost_rank_model(df_2023, seed=7, test_size=0.33)\n",
    "\n",
    "#sub_test = df_hurdle.loc[df_hurdle['id'] == i, ~df_hurdle.columns.isin(['rank', 'prediction_rel', 'predictions'])]\n",
    "\n",
    "out_all = ranked_model_predictions(model_ranked_all, df_hurdle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_all['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = out_all.copy()\n",
    "\n",
    "\n",
    "cols = [col for col in p1.columns if '_voting' in col]\n",
    "cols2 = [col for col in p1.columns if '_voting' not in col]\n",
    "\n",
    "\n",
    "# wide format _voting columns to long format\n",
    "p1 = pd.melt(p1, id_vars=cols2, \n",
    "                       value_vars= cols, var_name='from_code2')\n",
    "p1 = p1.loc[p1['value'] == 1]\n",
    "\n",
    "\n",
    "p1['from_code2'] = p1['from_code2'].str.replace('_voting', '').tolist()\n",
    "\n",
    "p1 = p1.loc[p1['value'] == 1]\n",
    "p1 = p1.drop('value', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "cols2 = [#'year', \n",
    "       'from_code2', #'to_code2', \n",
    "       #'rank',\n",
    "       'Contains_English', 'id',\n",
    "       'has_border', #'value', \n",
    "       'prediction_rel',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'Contains_Voting_Language', \n",
    "       'prop_emigrants_v2p',  'prop_emigrants_p2v', 'rank',\n",
    "       'comps_without_win', 'group', 'female', 'male', \n",
    "       'predictions', 'actual']\n",
    "\n",
    "cols = [col for col in p1.columns if col not in cols2]\n",
    "\n",
    "# # print p1.columns not in cols2\n",
    "print([col for col in p1.columns if col not in cols2])\n",
    "\n",
    "p1 = pd.melt(p1, id_vars=cols2, \n",
    "                       value_vars= cols, var_name='to_code2')\n",
    "p1 = p1.loc[p1['value'] == 1]\n",
    "# # drop value column\n",
    "p1 = p1.drop('value', axis=1)\n",
    "\n",
    "p1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1['total_points'] = p1.groupby('to_code2')['predictions'].transform('sum')\n",
    "\n",
    "p1['total_points'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "\n",
    "res = p1[['total_points', 'to_code2']].drop_duplicates()\n",
    "\n",
    "\n",
    "# match the country name to the country code\n",
    "res = res.merge(df[['from_code2', 'from_country']], left_on='to_code2', right_on='from_code2', how='left').drop_duplicates()\n",
    "\n",
    "res['position'] = res['total_points'].rank(ascending=False)\n",
    "\n",
    "# sort by total_points\n",
    "res = res.sort_values(by=['total_points'], ascending=False)\n",
    "\n",
    "# rename from country to country\n",
    "res = res.rename(columns={'from_country': 'country'})\n",
    "\n",
    "# remove from_code2 column\n",
    "res = res.drop('from_code2', axis=1)\n",
    "\n",
    "# print without index\n",
    "print(res.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
